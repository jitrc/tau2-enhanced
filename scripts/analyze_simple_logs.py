#!/usr/bin/env python3
"""
Analysis script for simplified execution logs from tau2-enhanced.

This script uses the new LogAnalyzer and LogVisualizer to process the
execution logs generated by the LoggingEnvironment and produces both
a console report and interactive HTML visualizations.
"""

import argparse
import json
from pathlib import Path
import sys

# Add the parent directory to the path to allow imports from tau2_enhanced
sys.path.append(str(Path(__file__).resolve().parents[1]))

from tau2_enhanced.analysis.analyzer import LogAnalyzer
from tau2_enhanced.analysis.visualizer import LogVisualizer
from tau2_enhanced.logging.events import ToolExecutionEvent


def convert_state_snapshots_to_events(state_snapshots):
    """
    Convert state snapshots to ToolExecutionEvent objects for analysis.

    Args:
        state_snapshots: List of state snapshot objects

    Returns:
        List of ToolExecutionEvent objects that can be analyzed by LogAnalyzer
    """
    events = []

    # Group snapshots by pairs (before/after tool calls)
    for i, snapshot in enumerate(state_snapshots):
        metadata = snapshot.get('metadata', {})
        tool_name = metadata.get('tool_name', 'unknown_tool')

        # Create a ToolExecutionEvent from state snapshot
        if 'before_' in snapshot.get('action_trigger', ''):
            # This is a before snapshot, create a mock ToolExecutionEvent
            event = ToolExecutionEvent(
                timestamp=snapshot.get('timestamp', 0),
                tool_name=tool_name,
                success=True,  # Assume success since we have state changes
                execution_time=0.1,  # Mock execution time
                requestor=metadata.get('requestor', 'assistant'),
                state_changed=True,  # State snapshots imply state change
                result_size=100,  # Mock result size
                args_count=metadata.get('args_count', 0),
                tool_call_id=f"{tool_name}_{i}",
                args_complexity_score=0.3,  # Mock complexity
                result_complexity_score=0.4,
                result_contains_errors=False,
                validation_errors=[]
            )
            events.append(event)

    print(f"   Converted {len(events)} state snapshots to ToolExecutionEvent objects.")
    return events


def analyze_logs(log_file: Path, output_dir: Path):
    """
    Loads, analyzes, and visualizes execution logs from a file.

    Args:
        log_file: Path to the JSON file containing the execution logs.
        output_dir: Directory to save the analysis reports and plots.
    """
    print(f"üìÅ Loading logs from: {log_file}")
    try:
        with log_file.open('r') as f:
            data = json.load(f)
    except (FileNotFoundError, json.JSONDecodeError) as e:
        print(f"‚ùå Error loading log file: {e}")
        return

    # The `simulations` key can be a list or a dictionary. We need to handle both.
    try:
        simulations_data = data.get('simulations', [])
        if isinstance(simulations_data, list) and simulations_data:
            first_simulation = simulations_data[0]
        elif isinstance(simulations_data, dict) and simulations_data:
            first_simulation = next(iter(simulations_data.values()))
        else:
            print("‚ùå 'simulations' field is empty or has an unexpected type.")
            return

        # The location of execution_logs can vary.
        # Try finding it inside `enhanced_logs` first, then fall back to the top level.
        log_data = None
        if 'enhanced_logs' in first_simulation and 'execution_logs' in first_simulation['enhanced_logs']:
            log_data = first_simulation['enhanced_logs']['execution_logs']
        elif 'execution_logs' in first_simulation:
            log_data = first_simulation['execution_logs']

        # If we found proper execution logs, use them directly
        if log_data:
            print(f"   Found {len(log_data)} execution events in enhanced logs.")
            # Convert list of dicts to wrapped structure that LogAnalyzer expects
            log_data = {'execution_events': log_data}

        # If no execution_logs found, check if we have state_snapshots to analyze
        if not log_data:
            state_snapshots = None
            if 'state_snapshots' in first_simulation:
                state_snapshots = first_simulation['state_snapshots']
            elif 'enhanced_logs' in first_simulation and 'state_snapshots' in first_simulation['enhanced_logs']:
                state_snapshots = first_simulation['enhanced_logs']['state_snapshots']

            if not state_snapshots:
                print("‚ùå Could not find 'execution_logs' or 'state_snapshots' in the first simulation.")
                print("   Please ensure you are using a results file with enhanced logging enabled.")
                return
            else:
                print("‚ö†Ô∏è  No execution_logs found, but state_snapshots are available.")
                print(f"   Found {len(state_snapshots)} state snapshots to analyze.")
                # Convert state snapshots to a format we can analyze
                log_data = convert_state_snapshots_to_events(state_snapshots)

    except (KeyError, IndexError, TypeError, StopIteration):
        print("‚ùå Could not find or parse the simulation data in the provided file.")
        print("   Please ensure you are using a valid results file.")
        return

    if not log_data:
        print("ü§∑ No log events found in the file.")
        return

    print(f"üî¨ Found {len(log_data)} log events to analyze.")
    output_dir.mkdir(parents=True, exist_ok=True)

    # 1. Analyze the logs
    analyzer = LogAnalyzer(log_data)

    print("\n" + "="*50)
    print("üìä SUMMARY METRICS")
    print("="*50)
    summary = analyzer.get_summary_metrics()
    for key, value in summary.items():
        if isinstance(value, float):
            print(f"  - {key.replace('_', ' ').title()}: {value:.3f}")
        else:
            print(f"  - {key.replace('_', ' ').title()}: {value}")

    print("\n" + "="*50)
    print("üõ†Ô∏è TOOL PERFORMANCE")
    print("="*50)
    tool_perf = analyzer.get_tool_performance()
    print(tool_perf.to_string(index=False))

    print("\n" + "="*50)
    print("üî• FAILURE ANALYSIS")
    print("="*50)
    failures = analyzer.get_failure_analysis()
    if not failures.empty:
        print(failures.to_string(index=False))
    else:
        print("  üéâ No failures recorded.")

    print("\n" + "="*50)
    print("üîÑ STATE CHANGE ANALYSIS")
    print("="*50)
    state_analysis = analyzer.get_state_change_analysis()
    if not state_analysis.empty:
        print(state_analysis.to_string(index=False))
    else:
        print("  ü§∑ No state change data available to analyze.")

    print("\n" + "="*50)
    print("üîó TOOL SEQUENCE ANALYSIS")
    print("="*50)
    sequence_analysis = analyzer.get_tool_sequence_analysis()
    if not sequence_analysis.empty:
        print("  Top 10 most common tool transitions:")
        print(sequence_analysis.head(10).to_string(index=False))
    else:
        print("  ü§∑ Not enough data to analyze tool sequences.")

    # 2. Visualize the results
    print("\n" + "="*50)
    print("üìà GENERATING VISUALIZATIONS")
    print("="*50)
    visualizer = LogVisualizer(analyzer)

    try:
        # Summary Dashboard
        summary_fig = visualizer.create_summary_dashboard()
        summary_path = output_dir / "summary_dashboard.html"
        summary_fig.write_html(summary_path)
        print(f"  ‚úÖ Summary dashboard saved to: {summary_path}")

        # Failure Analysis Plot
        failure_fig = visualizer.create_failure_analysis_plot()
        failure_path = output_dir / "failure_analysis.html"
        failure_fig.write_html(failure_path)
        print(f"  ‚úÖ Failure analysis plot saved to: {failure_path}")

        # State Change Plot
        state_change_fig = visualizer.create_state_change_plot()
        state_change_path = output_dir / "state_change_analysis.html"
        state_change_fig.write_html(state_change_path)
        print(f"  ‚úÖ State change analysis plot saved to: {state_change_path}")

        # Tool Flow Sankey
        sankey_fig = visualizer.create_tool_flow_sankey()
        sankey_path = output_dir / "tool_flow_sankey.html"
        sankey_fig.write_html(sankey_path)
        print(f"  ‚úÖ Tool flow Sankey diagram saved to: {sankey_path}")

        # Bottleneck Plot
        bottleneck_fig = visualizer.create_performance_bottleneck_plot()
        bottleneck_path = output_dir / "performance_bottlenecks.html"
        bottleneck_fig.write_html(bottleneck_path)
        print(f"  ‚úÖ Performance bottleneck plot saved to: {bottleneck_path}")

        # Comprehensive Report
        report_path = output_dir / "report.html"
        visualizer.create_comprehensive_report(str(report_path), log_file.name)
        print(f"  ‚úÖ Comprehensive report saved to: {report_path}")

    except Exception as e:
        print(f"  ‚ùå Error during visualization: {e}")

    print("\nüéâ Analysis complete!")
    print(f"\nüìÑ For a comprehensive overview, open: {output_dir / 'report.html'}")


def main():
    parser = argparse.ArgumentParser(
        description="Analyze simplified execution logs from tau2-enhanced."
    )
    parser.add_argument(
        "log_file",
        type=Path,
        help="Path to the JSON results file containing execution_logs."
    )
    parser.add_argument(
        "-o", "--output",
        type=Path,
        default=Path("analysis_results"),
        help="Base directory to save analysis plots."
    )
    args = parser.parse_args()

    # Create a subfolder in the output directory named after the input file
    output_subdir = args.output / args.log_file.stem
    print(f"üíæ Output will be saved to: {output_subdir}")

    analyze_logs(args.log_file, output_subdir)


if __name__ == "__main__":
    main()
