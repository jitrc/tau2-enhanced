#!/usr/bin/env python3
"""
Analysis script for simplified execution logs from tau2-enhanced.

This script uses the new LogAnalyzer and LogVisualizer to process the
execution logs generated by the LoggingEnvironment and produces both
a console report and interactive HTML visualizations.
"""

import argparse
import json
from pathlib import Path
import sys

# Add the parent directory to the path to allow imports from tau2_enhanced
sys.path.append(str(Path(__file__).resolve().parents[1]))

from tau2_enhanced.analysis.analyzer import LogAnalyzer
from tau2_enhanced.analysis.visualizer import LogVisualizer
from tau2_enhanced.logging.events import ToolExecutionEvent


def convert_state_snapshots_to_events(state_snapshots):
    """
    Convert state snapshots to ToolExecutionEvent objects for analysis.

    Args:
        state_snapshots: List of state snapshot objects

    Returns:
        List of ToolExecutionEvent objects that can be analyzed by LogAnalyzer
    """
    events = []

    # Group snapshots by pairs (before/after tool calls)
    for i, snapshot in enumerate(state_snapshots):
        metadata = snapshot.get('metadata', {})
        tool_name = metadata.get('tool_name', 'unknown_tool')

        # Create a ToolExecutionEvent from state snapshot
        if 'before_' in snapshot.get('action_trigger', ''):
            # This is a before snapshot, create a mock ToolExecutionEvent
            event = ToolExecutionEvent(
                timestamp=snapshot.get('timestamp', 0),
                tool_name=tool_name,
                success=True,  # Assume success since we have state changes
                execution_time=0.1,  # Mock execution time
                requestor=metadata.get('requestor', 'assistant'),
                state_changed=True,  # State snapshots imply state change
                result_size=100,  # Mock result size
                args_count=metadata.get('args_count', 0),
                tool_call_id=f"{tool_name}_{i}",
                args_complexity_score=0.3,  # Mock complexity
                result_complexity_score=0.4,
                result_contains_errors=False,
                validation_errors=[]
            )
            events.append(event)

    print(f"   Converted {len(events)} state snapshots to ToolExecutionEvent objects.")
    return events


def analyze_logs(log_file: Path, output_dir: Path):
    """
    Loads, analyzes, and visualizes execution logs from a file.
    """
    print(f"üìÅ Loading logs from: {log_file}")

    try:
        with log_file.open('r') as f:
            data = json.load(f)
        print(f"  ‚úÖ Successfully loaded log file")
    except (FileNotFoundError, json.JSONDecodeError) as e:
        print(f"‚ùå Error loading log file: {e}")
        return

    # Detect log format and handle accordingly
    if 'execution_events' in data:
        # New simplified format: direct execution_events array
        print("  üìä Detected new simplified enhanced logs format")

        # Try to load the corresponding standard results file for simulation success data
        if '_enhanced_logs' in log_file.name:
            standard_file = Path(str(log_file).replace('_enhanced_logs', ''))
            try:
                with standard_file.open('r') as f:
                    standard_data = json.load(f)
                print(f"  ‚úÖ Loaded standard results file for simulation data")

                # Add simulation data to the enhanced logs for analysis
                data['simulations'] = standard_data.get('simulations', [])

            except (FileNotFoundError, json.JSONDecodeError) as e:
                print(f"  ‚ö†Ô∏è  Could not load standard results file: {e}")

        analyzer_data = data  # Use combined data
    elif 'simulations' in data:
        # Legacy format: simulations with embedded logs
        print("  üìä Detected legacy enhanced logs format")
        analyzer_data = data
    else:
        print("‚ùå Unrecognized log format - no execution_events or simulations found")
        return

    # The LogAnalyzer is capable of handling different log formats.
    # We pass the entire loaded JSON data to it.
    analyzer = LogAnalyzer(analyzer_data)

    # Check if any tool events were found
    if analyzer.df.empty:
        print("ü§∑ No tool execution events found in the file.")
        return

    print(f"üî¨ Found {len(analyzer.df)} tool execution events to analyze.")
    output_dir.mkdir(parents=True, exist_ok=True)

    # 1. Analyze the logs
    print("\n" + "="*50)
    print("üìä SUMMARY METRICS")
    print("="*50)
    summary = analyzer.get_summary_metrics()
    for key, value in summary.items():
        if isinstance(value, float):
            print(f"  - {key.replace('_', ' ').title()}: {value:.3f}")
        else:
            print(f"  - {key.replace('_', ' ').title()}: {value}")

    print("\n" + "="*50)
    print("üõ†Ô∏è TOOL PERFORMANCE")
    print("="*50)
    tool_perf = analyzer.get_tool_performance()
    print(tool_perf.to_string(index=False))

    print("\n" + "="*50)
    print("üî• FAILURE ANALYSIS")
    print("="*50)
    failures = analyzer.get_failure_analysis()
    if not failures.empty:
        print(failures.to_string(index=False))
    else:
        print("  üéâ No failures recorded.")

    print("\n" + "="*50)
    print("üîÑ STATE CHANGE ANALYSIS")
    print("="*50)
    state_analysis = analyzer.get_state_change_analysis()
    if not state_analysis.empty:
        print(state_analysis.to_string(index=False))
    else:
        print("  ü§∑ No state change data available to analyze.")

    print("\n" + "="*50)
    print("üîó TOOL SEQUENCE ANALYSIS")
    print("="*50)
    sequence_analysis = analyzer.get_tool_sequence_analysis()
    if not sequence_analysis.empty:
        print("  Top 10 most common tool transitions:")
        print(sequence_analysis.head(10).to_string(index=False))
    else:
        print("  ü§∑ Not enough data to analyze tool sequences.")

    # 2. Visualize the results
    print("\n" + "="*50)
    print("üìà GENERATING VISUALIZATIONS")
    print("="*50)
    visualizer = LogVisualizer(analyzer)

    try:
        # Summary Dashboard
        summary_fig = visualizer.create_summary_dashboard()
        summary_path = output_dir / "summary_dashboard.html"
        summary_fig.write_html(summary_path)
        print(f"  ‚úÖ Summary dashboard saved to: {summary_path}")

        # Failure Analysis Plot
        failure_fig = visualizer.create_failure_analysis_plot()
        failure_path = output_dir / "failure_analysis.html"
        failure_fig.write_html(failure_path)
        print(f"  ‚úÖ Failure analysis plot saved to: {failure_path}")

        # State Change Plot
        state_change_fig = visualizer.create_state_change_plot()
        state_change_path = output_dir / "state_change_analysis.html"
        state_change_fig.write_html(state_change_path)
        print(f"  ‚úÖ State change analysis plot saved to: {state_change_path}")

        # Tool Flow Sankey
        sankey_fig = visualizer.create_tool_flow_sankey()
        sankey_path = output_dir / "tool_flow_sankey.html"
        sankey_fig.write_html(sankey_path)
        print(f"  ‚úÖ Tool flow Sankey diagram saved to: {sankey_path}")

        # Bottleneck Plot
        bottleneck_fig = visualizer.create_performance_bottleneck_plot()
        bottleneck_path = output_dir / "performance_bottlenecks.html"
        bottleneck_fig.write_html(bottleneck_path)
        print(f"  ‚úÖ Performance bottleneck plot saved to: {bottleneck_path}")

        # Comprehensive Tool Report
        tool_report_path = output_dir / "tool_report.html"
        visualizer.create_tool_report(str(tool_report_path), log_file.name)
        print(f"  ‚úÖ Comprehensive tool report saved to: {tool_report_path}")

        # Comprehensive Simulation Report
        report_path = output_dir / "report.html"
        visualizer.create_comprehensive_report(str(report_path), log_file.name)
        print(f"  ‚úÖ Comprehensive simulation report saved to: {report_path}")

        # Markdown Report
        markdown_path = output_dir / "analysis_report.md"
        visualizer.create_markdown_report(str(markdown_path), log_file.name)
        print(f"  ‚úÖ Markdown analysis report saved to: {markdown_path}")

    except Exception as e:
        print(f"  ‚ùå Error during visualization: {e}")

    print("\nüéâ Analysis complete!")
    print(f"\nüìÑ For a comprehensive overview, open: {output_dir / 'report.html'}")
    print(f"üõ†Ô∏è For a detailed tool analysis, open: {output_dir / 'tool_report.html'}")
    print(f"üìã For a markdown summary report, see: {output_dir / 'analysis_report.md'}")


def main():
    parser = argparse.ArgumentParser(
        description="Analyze simplified execution logs from tau2-enhanced."
    )
    parser.add_argument(
        "log_file",
        type=Path,
        help="Path to the JSON results file containing execution_logs."
    )
    parser.add_argument(
        "-o", "--output",
        type=Path,
        default=Path("analysis_results"),
        help="Base directory to save analysis plots."
    )
    args = parser.parse_args()

    # Create a subfolder in the output directory named after the input file
    output_subdir = args.output / args.log_file.stem
    print(f"üíæ Output will be saved to: {output_subdir}")

    analyze_logs(args.log_file, output_subdir)


if __name__ == "__main__":
    main()
